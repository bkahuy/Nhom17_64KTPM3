{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thư viện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from fastapi import FastAPI\n",
    "from fastapi.responses import HTMLResponse\n",
    "from pydantic import BaseModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits import mplot3d\n",
    "import io, os\n",
    "from numpy import double\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"giavang1.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "không hiểu cột change % là gì lên drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Change %'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Price</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Vol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12/30/2022</td>\n",
       "      <td>1,826.20</td>\n",
       "      <td>1,821.80</td>\n",
       "      <td>1,832.40</td>\n",
       "      <td>1,819.80</td>\n",
       "      <td>107.50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12/29/2022</td>\n",
       "      <td>1,826.00</td>\n",
       "      <td>1,812.30</td>\n",
       "      <td>1,827.30</td>\n",
       "      <td>1,811.20</td>\n",
       "      <td>105.99K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12/28/2022</td>\n",
       "      <td>1,815.80</td>\n",
       "      <td>1,822.40</td>\n",
       "      <td>1,822.80</td>\n",
       "      <td>1,804.20</td>\n",
       "      <td>118.08K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12/27/2022</td>\n",
       "      <td>1,823.10</td>\n",
       "      <td>1,808.20</td>\n",
       "      <td>1,841.90</td>\n",
       "      <td>1,808.00</td>\n",
       "      <td>159.62K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12/26/2022</td>\n",
       "      <td>1,809.70</td>\n",
       "      <td>1,805.80</td>\n",
       "      <td>1,811.95</td>\n",
       "      <td>1,805.55</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date     Price      Open      High       Low      Vol\n",
       "0  12/30/2022  1,826.20  1,821.80  1,832.40  1,819.80  107.50K\n",
       "1  12/29/2022  1,826.00  1,812.30  1,827.30  1,811.20  105.99K\n",
       "2  12/28/2022  1,815.80  1,822.40  1,822.80  1,804.20  118.08K\n",
       "3  12/27/2022  1,823.10  1,808.20  1,841.90  1,808.00  159.62K\n",
       "4  12/26/2022  1,809.70  1,805.80  1,811.95  1,805.55      NaN"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loại bỏ hàng có ít nhất một dòng dữ liệu lỗi/NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Price</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Vol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12/30/2022</td>\n",
       "      <td>1,826.20</td>\n",
       "      <td>1,821.80</td>\n",
       "      <td>1,832.40</td>\n",
       "      <td>1,819.80</td>\n",
       "      <td>107.50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12/29/2022</td>\n",
       "      <td>1,826.00</td>\n",
       "      <td>1,812.30</td>\n",
       "      <td>1,827.30</td>\n",
       "      <td>1,811.20</td>\n",
       "      <td>105.99K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12/28/2022</td>\n",
       "      <td>1,815.80</td>\n",
       "      <td>1,822.40</td>\n",
       "      <td>1,822.80</td>\n",
       "      <td>1,804.20</td>\n",
       "      <td>118.08K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12/27/2022</td>\n",
       "      <td>1,823.10</td>\n",
       "      <td>1,808.20</td>\n",
       "      <td>1,841.90</td>\n",
       "      <td>1,808.00</td>\n",
       "      <td>159.62K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12/23/2022</td>\n",
       "      <td>1,804.20</td>\n",
       "      <td>1,801.00</td>\n",
       "      <td>1,812.20</td>\n",
       "      <td>1,798.90</td>\n",
       "      <td>105.46K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date     Price      Open      High       Low      Vol\n",
       "0  12/30/2022  1,826.20  1,821.80  1,832.40  1,819.80  107.50K\n",
       "1  12/29/2022  1,826.00  1,812.30  1,827.30  1,811.20  105.99K\n",
       "2  12/28/2022  1,815.80  1,822.40  1,822.80  1,804.20  118.08K\n",
       "3  12/27/2022  1,823.10  1,808.20  1,841.90  1,808.00  159.62K\n",
       "5  12/23/2022  1,804.20  1,801.00  1,812.20  1,798.90  105.46K"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chuyển đổi cột date sang dạng date nếu lỗi trả về NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Price</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Vol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>1,826.20</td>\n",
       "      <td>1,821.80</td>\n",
       "      <td>1,832.40</td>\n",
       "      <td>1,819.80</td>\n",
       "      <td>107.50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-12-29</td>\n",
       "      <td>1,826.00</td>\n",
       "      <td>1,812.30</td>\n",
       "      <td>1,827.30</td>\n",
       "      <td>1,811.20</td>\n",
       "      <td>105.99K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-12-28</td>\n",
       "      <td>1,815.80</td>\n",
       "      <td>1,822.40</td>\n",
       "      <td>1,822.80</td>\n",
       "      <td>1,804.20</td>\n",
       "      <td>118.08K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-12-27</td>\n",
       "      <td>1,823.10</td>\n",
       "      <td>1,808.20</td>\n",
       "      <td>1,841.90</td>\n",
       "      <td>1,808.00</td>\n",
       "      <td>159.62K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022-12-23</td>\n",
       "      <td>1,804.20</td>\n",
       "      <td>1,801.00</td>\n",
       "      <td>1,812.20</td>\n",
       "      <td>1,798.90</td>\n",
       "      <td>105.46K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date     Price      Open      High       Low      Vol\n",
       "0 2022-12-30  1,826.20  1,821.80  1,832.40  1,819.80  107.50K\n",
       "1 2022-12-29  1,826.00  1,812.30  1,827.30  1,811.20  105.99K\n",
       "2 2022-12-28  1,815.80  1,822.40  1,822.80  1,804.20  118.08K\n",
       "3 2022-12-27  1,823.10  1,808.20  1,841.90  1,808.00  159.62K\n",
       "5 2022-12-23  1,804.20  1,801.00  1,812.20  1,798.90  105.46K"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "df.dropna(inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Price</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Vol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>1826.2</td>\n",
       "      <td>1821.8</td>\n",
       "      <td>1832.4</td>\n",
       "      <td>1819.8</td>\n",
       "      <td>107500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-12-29</td>\n",
       "      <td>1826.0</td>\n",
       "      <td>1812.3</td>\n",
       "      <td>1827.3</td>\n",
       "      <td>1811.2</td>\n",
       "      <td>105990.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-12-28</td>\n",
       "      <td>1815.8</td>\n",
       "      <td>1822.4</td>\n",
       "      <td>1822.8</td>\n",
       "      <td>1804.2</td>\n",
       "      <td>118080.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-12-27</td>\n",
       "      <td>1823.1</td>\n",
       "      <td>1808.2</td>\n",
       "      <td>1841.9</td>\n",
       "      <td>1808.0</td>\n",
       "      <td>159620.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022-12-23</td>\n",
       "      <td>1804.2</td>\n",
       "      <td>1801.0</td>\n",
       "      <td>1812.2</td>\n",
       "      <td>1798.9</td>\n",
       "      <td>105460.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date   Price    Open    High     Low       Vol\n",
       "0 2022-12-30  1826.2  1821.8  1832.4  1819.8  107500.0\n",
       "1 2022-12-29  1826.0  1812.3  1827.3  1811.2  105990.0\n",
       "2 2022-12-28  1815.8  1822.4  1822.8  1804.2  118080.0\n",
       "3 2022-12-27  1823.1  1808.2  1841.9  1808.0  159620.0\n",
       "5 2022-12-23  1804.2  1801.0  1812.2  1798.9  105460.0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['Price', 'Open', 'High', 'Low']] = df[['Price', 'Open', 'High', 'Low']].replace({',': ''}, regex=True)\n",
    "df['Vol'] = df['Vol'].replace({'K': ''}, regex=True).astype(double) * 1000\n",
    "x = df[['Open', 'High','Low' ,'Vol']].values\n",
    "y = df['Price'].values\n",
    "scaler = StandardScaler()\n",
    "x_scaled = scaler.fit_transform(x)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hồi quy tuyến tính"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LinearRegression()\n",
    "\n",
    "lr_model.fit(x_train, y_train)\n",
    "y_pred_lr = lr_model.predict(x_test)\n",
    "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
    "r2_lr = r2_score(y_test, y_pred_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hồi quy lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\buikh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.563e+04, tolerance: 1.181e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "lasso_model = Lasso(alpha=0.1)\n",
    "\n",
    "lasso_model.fit(x_train, y_train)\n",
    "y_pred_lasso = lasso_model.predict(x_test)\n",
    "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
    "r2_lasso = r2_score(y_test, y_pred_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nerral network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1065531.90131677\n",
      "Iteration 2, loss = 1064283.15272819\n",
      "Iteration 3, loss = 1062827.41759930\n",
      "Iteration 4, loss = 1060982.58425191\n",
      "Iteration 5, loss = 1058462.61092230\n",
      "Iteration 6, loss = 1055098.34046220\n",
      "Iteration 7, loss = 1050665.76321365\n",
      "Iteration 8, loss = 1044849.83657125\n",
      "Iteration 9, loss = 1037371.73369276\n",
      "Iteration 10, loss = 1027936.62692898\n",
      "Iteration 11, loss = 1015998.30234802\n",
      "Iteration 12, loss = 1001794.87513436\n",
      "Iteration 13, loss = 985155.53304701\n",
      "Iteration 14, loss = 965273.85429238\n",
      "Iteration 15, loss = 942690.05090211\n",
      "Iteration 16, loss = 916750.94183080\n",
      "Iteration 17, loss = 886825.33593660\n",
      "Iteration 18, loss = 853073.56854961\n",
      "Iteration 19, loss = 815459.41044039\n",
      "Iteration 20, loss = 774865.30494227\n",
      "Iteration 21, loss = 731664.00426455\n",
      "Iteration 22, loss = 684939.50859010\n",
      "Iteration 23, loss = 636610.18642515\n",
      "Iteration 24, loss = 587899.69692618\n",
      "Iteration 25, loss = 538632.94312335\n",
      "Iteration 26, loss = 488234.94597555\n",
      "Iteration 27, loss = 438826.33917105\n",
      "Iteration 28, loss = 391775.86698539\n",
      "Iteration 29, loss = 347313.09262557\n",
      "Iteration 30, loss = 305046.53996225\n",
      "Iteration 31, loss = 266204.44280191\n",
      "Iteration 32, loss = 231139.88645205\n",
      "Iteration 33, loss = 199501.16926814\n",
      "Iteration 34, loss = 171936.71806561\n",
      "Iteration 35, loss = 148337.14068010\n",
      "Iteration 36, loss = 128986.91944962\n",
      "Iteration 37, loss = 113133.19680924\n",
      "Iteration 38, loss = 100534.11343399\n",
      "Iteration 39, loss = 90301.84140692\n",
      "Iteration 40, loss = 82227.34782076\n",
      "Iteration 41, loss = 75901.33730500\n",
      "Iteration 42, loss = 71207.14973548\n",
      "Iteration 43, loss = 67471.39920741\n",
      "Iteration 44, loss = 64193.88606004\n",
      "Iteration 45, loss = 61695.88100843\n",
      "Iteration 46, loss = 59584.56899457\n",
      "Iteration 47, loss = 57814.61809482\n",
      "Iteration 48, loss = 56324.02073918\n",
      "Iteration 49, loss = 55022.20355424\n",
      "Iteration 50, loss = 53902.98774161\n",
      "Iteration 51, loss = 52894.47988518\n",
      "Iteration 52, loss = 51942.22878482\n",
      "Iteration 53, loss = 51075.62622975\n",
      "Iteration 54, loss = 50202.16372703\n",
      "Iteration 55, loss = 49312.62462704\n",
      "Iteration 56, loss = 48450.65102823\n",
      "Iteration 57, loss = 47639.46214925\n",
      "Iteration 58, loss = 46872.80945165\n",
      "Iteration 59, loss = 46091.10069554\n",
      "Iteration 60, loss = 45307.92570452\n",
      "Iteration 61, loss = 44552.20489132\n",
      "Iteration 62, loss = 43783.32092727\n",
      "Iteration 63, loss = 43021.76329214\n",
      "Iteration 64, loss = 42275.24938730\n",
      "Iteration 65, loss = 41493.80464638\n",
      "Iteration 66, loss = 40669.89882827\n",
      "Iteration 67, loss = 39868.38728948\n",
      "Iteration 68, loss = 39100.28414903\n",
      "Iteration 69, loss = 38353.54692811\n",
      "Iteration 70, loss = 37585.72698923\n",
      "Iteration 71, loss = 36792.54560497\n",
      "Iteration 72, loss = 36037.26401333\n",
      "Iteration 73, loss = 35345.57623168\n",
      "Iteration 74, loss = 34687.29444233\n",
      "Iteration 75, loss = 34065.84235212\n",
      "Iteration 76, loss = 33413.00059187\n",
      "Iteration 77, loss = 32784.54886738\n",
      "Iteration 78, loss = 32163.37103035\n",
      "Iteration 79, loss = 31529.24395226\n",
      "Iteration 80, loss = 30888.07378035\n",
      "Iteration 81, loss = 30218.69809787\n",
      "Iteration 82, loss = 29572.36065946\n",
      "Iteration 83, loss = 28924.01930307\n",
      "Iteration 84, loss = 28306.66689691\n",
      "Iteration 85, loss = 27746.35661663\n",
      "Iteration 86, loss = 27167.82594895\n",
      "Iteration 87, loss = 26610.20925452\n",
      "Iteration 88, loss = 26090.20540410\n",
      "Iteration 89, loss = 25590.82266553\n",
      "Iteration 90, loss = 25084.63430551\n",
      "Iteration 91, loss = 24549.85631760\n",
      "Iteration 92, loss = 24072.52726439\n",
      "Iteration 93, loss = 23522.41473721\n",
      "Iteration 94, loss = 22969.04883065\n",
      "Iteration 95, loss = 22439.66422233\n",
      "Iteration 96, loss = 21916.65073122\n",
      "Iteration 97, loss = 21385.79309475\n",
      "Iteration 98, loss = 20866.88091280\n",
      "Iteration 99, loss = 20390.88687936\n",
      "Iteration 100, loss = 19915.41574421\n",
      "Iteration 101, loss = 19444.47804109\n",
      "Iteration 102, loss = 18961.22682198\n",
      "Iteration 103, loss = 18491.41304556\n",
      "Iteration 104, loss = 18040.40760701\n",
      "Iteration 105, loss = 17626.27571760\n",
      "Iteration 106, loss = 17209.19486889\n",
      "Iteration 107, loss = 16787.85853754\n",
      "Iteration 108, loss = 16359.17234229\n",
      "Iteration 109, loss = 15973.83730494\n",
      "Iteration 110, loss = 15548.83327408\n",
      "Iteration 111, loss = 15125.00686179\n",
      "Iteration 112, loss = 14732.78998072\n",
      "Iteration 113, loss = 14346.82973005\n",
      "Iteration 114, loss = 13993.23190728\n",
      "Iteration 115, loss = 13631.54677358\n",
      "Iteration 116, loss = 13285.43581225\n",
      "Iteration 117, loss = 12943.01727022\n",
      "Iteration 118, loss = 12595.08452630\n",
      "Iteration 119, loss = 12257.52850110\n",
      "Iteration 120, loss = 11942.38413833\n",
      "Iteration 121, loss = 11630.74827428\n",
      "Iteration 122, loss = 11299.22923230\n",
      "Iteration 123, loss = 10980.93486503\n",
      "Iteration 124, loss = 10667.99760283\n",
      "Iteration 125, loss = 10390.44546709\n",
      "Iteration 126, loss = 10143.92494794\n",
      "Iteration 127, loss = 9880.74868214\n",
      "Iteration 128, loss = 9627.30054869\n",
      "Iteration 129, loss = 9383.13253141\n",
      "Iteration 130, loss = 9148.87696616\n",
      "Iteration 131, loss = 8913.22341297\n",
      "Iteration 132, loss = 8705.51255316\n",
      "Iteration 133, loss = 8518.53341200\n",
      "Iteration 134, loss = 8288.41935832\n",
      "Iteration 135, loss = 8062.51585272\n",
      "Iteration 136, loss = 7853.93800921\n",
      "Iteration 137, loss = 7640.96641108\n",
      "Iteration 138, loss = 7446.63849511\n",
      "Iteration 139, loss = 7261.04635899\n",
      "Iteration 140, loss = 7065.93108931\n",
      "Iteration 141, loss = 6904.12892403\n",
      "Iteration 142, loss = 6738.12550075\n",
      "Iteration 143, loss = 6577.14010088\n",
      "Iteration 144, loss = 6416.57154607\n",
      "Iteration 145, loss = 6261.95915160\n",
      "Iteration 146, loss = 6106.96069124\n",
      "Iteration 147, loss = 5953.00906763\n",
      "Iteration 148, loss = 5796.46266768\n",
      "Iteration 149, loss = 5640.06244018\n",
      "Iteration 150, loss = 5513.22042806\n",
      "Iteration 151, loss = 5391.80902267\n",
      "Iteration 152, loss = 5234.10314172\n",
      "Iteration 153, loss = 5086.79722930\n",
      "Iteration 154, loss = 4967.71555953\n",
      "Iteration 155, loss = 4852.66054937\n",
      "Iteration 156, loss = 4735.66660774\n",
      "Iteration 157, loss = 4623.03691023\n",
      "Iteration 158, loss = 4516.20682713\n",
      "Iteration 159, loss = 4412.45353704\n",
      "Iteration 160, loss = 4307.14899259\n",
      "Iteration 161, loss = 4213.62324137\n",
      "Iteration 162, loss = 4115.62867019\n",
      "Iteration 163, loss = 4024.66224081\n",
      "Iteration 164, loss = 3946.52226296\n",
      "Iteration 165, loss = 3863.54744036\n",
      "Iteration 166, loss = 3764.77214866\n",
      "Iteration 167, loss = 3678.12735958\n",
      "Iteration 168, loss = 3592.62441237\n",
      "Iteration 169, loss = 3519.15670895\n",
      "Iteration 170, loss = 3473.65280461\n",
      "Iteration 171, loss = 3363.02452645\n",
      "Iteration 172, loss = 3263.50355792\n",
      "Iteration 173, loss = 3181.07730436\n",
      "Iteration 174, loss = 3113.67690300\n",
      "Iteration 175, loss = 3052.09641428\n",
      "Iteration 176, loss = 2987.26598428\n",
      "Iteration 177, loss = 2932.22027378\n",
      "Iteration 178, loss = 2863.97232926\n",
      "Iteration 179, loss = 2813.68967480\n",
      "Iteration 180, loss = 2753.14710256\n",
      "Iteration 181, loss = 2681.75138467\n",
      "Iteration 182, loss = 2619.98520280\n",
      "Iteration 183, loss = 2563.48546420\n",
      "Iteration 184, loss = 2517.22525669\n",
      "Iteration 185, loss = 2472.83552519\n",
      "Iteration 186, loss = 2421.79283827\n",
      "Iteration 187, loss = 2374.51138904\n",
      "Iteration 188, loss = 2328.22274437\n",
      "Iteration 189, loss = 2287.87056615\n",
      "Iteration 190, loss = 2249.18701880\n",
      "Iteration 191, loss = 2209.37266570\n",
      "Iteration 192, loss = 2170.14836515\n",
      "Iteration 193, loss = 2133.46373739\n",
      "Iteration 194, loss = 2093.18999622\n",
      "Iteration 195, loss = 2059.36276553\n",
      "Iteration 196, loss = 2024.56676648\n",
      "Iteration 197, loss = 1989.16427157\n",
      "Iteration 198, loss = 1955.55055372\n",
      "Iteration 199, loss = 1919.78652175\n",
      "Iteration 200, loss = 1886.31064969\n",
      "Iteration 201, loss = 1856.82581546\n",
      "Iteration 202, loss = 1826.62348226\n",
      "Iteration 203, loss = 1795.10755411\n",
      "Iteration 204, loss = 1764.47473951\n",
      "Iteration 205, loss = 1737.18256828\n",
      "Iteration 206, loss = 1710.94447831\n",
      "Iteration 207, loss = 1680.67351933\n",
      "Iteration 208, loss = 1651.00373509\n",
      "Iteration 209, loss = 1624.90372484\n",
      "Iteration 210, loss = 1597.24331077\n",
      "Iteration 211, loss = 1570.83979911\n",
      "Iteration 212, loss = 1546.18821843\n",
      "Iteration 213, loss = 1520.45742516\n",
      "Iteration 214, loss = 1499.72157038\n",
      "Iteration 215, loss = 1475.15207265\n",
      "Iteration 216, loss = 1450.61174441\n",
      "Iteration 217, loss = 1428.03983694\n",
      "Iteration 218, loss = 1402.36859492\n",
      "Iteration 219, loss = 1377.95568212\n",
      "Iteration 220, loss = 1355.54720980\n",
      "Iteration 221, loss = 1331.93703235\n",
      "Iteration 222, loss = 1309.91189729\n",
      "Iteration 223, loss = 1287.58813867\n",
      "Iteration 224, loss = 1267.49092888\n",
      "Iteration 225, loss = 1247.91959511\n",
      "Iteration 226, loss = 1228.71149270\n",
      "Iteration 227, loss = 1206.58472924\n",
      "Iteration 228, loss = 1190.50951651\n",
      "Iteration 229, loss = 1170.47322846\n",
      "Iteration 230, loss = 1149.46347921\n",
      "Iteration 231, loss = 1126.05128378\n",
      "Iteration 232, loss = 1108.03212082\n",
      "Iteration 233, loss = 1091.51856545\n",
      "Iteration 234, loss = 1072.25960621\n",
      "Iteration 235, loss = 1054.64662259\n",
      "Iteration 236, loss = 1036.79793614\n",
      "Iteration 237, loss = 1017.11818188\n",
      "Iteration 238, loss = 1006.15897217\n",
      "Iteration 239, loss = 988.21397678\n",
      "Iteration 240, loss = 971.11747398\n",
      "Iteration 241, loss = 954.84312465\n",
      "Iteration 242, loss = 941.65881448\n",
      "Iteration 243, loss = 931.17982209\n",
      "Iteration 244, loss = 912.83899109\n",
      "Iteration 245, loss = 892.52147239\n",
      "Iteration 246, loss = 873.50631265\n",
      "Iteration 247, loss = 859.33429464\n",
      "Iteration 248, loss = 843.37426406\n",
      "Iteration 249, loss = 831.77080292\n",
      "Iteration 250, loss = 816.43160969\n",
      "Iteration 251, loss = 801.04632693\n",
      "Iteration 252, loss = 787.40751997\n",
      "Iteration 253, loss = 771.74820329\n",
      "Iteration 254, loss = 758.18888129\n",
      "Iteration 255, loss = 747.01421772\n",
      "Iteration 256, loss = 736.29251443\n",
      "Iteration 257, loss = 724.96632069\n",
      "Iteration 258, loss = 718.05906330\n",
      "Iteration 259, loss = 707.17729046\n",
      "Iteration 260, loss = 691.06581256\n",
      "Iteration 261, loss = 683.42720210\n",
      "Iteration 262, loss = 669.88956454\n",
      "Iteration 263, loss = 657.65094360\n",
      "Iteration 264, loss = 647.43004111\n",
      "Iteration 265, loss = 638.46872104\n",
      "Iteration 266, loss = 628.01909423\n",
      "Iteration 267, loss = 618.79479915\n",
      "Iteration 268, loss = 609.27406185\n",
      "Iteration 269, loss = 598.46526159\n",
      "Iteration 270, loss = 585.26024012\n",
      "Iteration 271, loss = 574.21295298\n",
      "Iteration 272, loss = 565.96168149\n",
      "Iteration 273, loss = 558.38569234\n",
      "Iteration 274, loss = 554.91503162\n",
      "Iteration 275, loss = 539.92963751\n",
      "Iteration 276, loss = 526.81282049\n",
      "Iteration 277, loss = 519.10470746\n",
      "Iteration 278, loss = 514.05775082\n",
      "Iteration 279, loss = 504.75037437\n",
      "Iteration 280, loss = 498.50935310\n",
      "Iteration 281, loss = 489.44443369\n",
      "Iteration 282, loss = 482.55508235\n",
      "Iteration 283, loss = 476.61610582\n",
      "Iteration 284, loss = 468.85736660\n",
      "Iteration 285, loss = 461.82607848\n",
      "Iteration 286, loss = 456.06273597\n",
      "Iteration 287, loss = 449.41536384\n",
      "Iteration 288, loss = 444.12261551\n",
      "Iteration 289, loss = 439.20779559\n",
      "Iteration 290, loss = 430.70098495\n",
      "Iteration 291, loss = 423.90565129\n",
      "Iteration 292, loss = 418.99721332\n",
      "Iteration 293, loss = 413.21996808\n",
      "Iteration 294, loss = 406.53242664\n",
      "Iteration 295, loss = 400.96117384\n",
      "Iteration 296, loss = 395.65375831\n",
      "Iteration 297, loss = 391.23816619\n",
      "Iteration 298, loss = 385.02939173\n",
      "Iteration 299, loss = 380.50565036\n",
      "Iteration 300, loss = 375.03609262\n",
      "Iteration 301, loss = 370.04080708\n",
      "Iteration 302, loss = 365.66818150\n",
      "Iteration 303, loss = 361.47949976\n",
      "Iteration 304, loss = 354.99748290\n",
      "Iteration 305, loss = 349.61148584\n",
      "Iteration 306, loss = 345.21784809\n",
      "Iteration 307, loss = 341.16818490\n",
      "Iteration 308, loss = 336.98501805\n",
      "Iteration 309, loss = 332.67413858\n",
      "Iteration 310, loss = 328.93719402\n",
      "Iteration 311, loss = 324.28466605\n",
      "Iteration 312, loss = 319.29639866\n",
      "Iteration 313, loss = 315.63397724\n",
      "Iteration 314, loss = 313.88847014\n",
      "Iteration 315, loss = 309.38504904\n",
      "Iteration 316, loss = 302.65202858\n",
      "Iteration 317, loss = 300.07038052\n",
      "Iteration 318, loss = 296.27779326\n",
      "Iteration 319, loss = 291.09100832\n",
      "Iteration 320, loss = 287.60590136\n",
      "Iteration 321, loss = 284.96923653\n",
      "Iteration 322, loss = 281.05067804\n",
      "Iteration 323, loss = 277.40587898\n",
      "Iteration 324, loss = 275.13622557\n",
      "Iteration 325, loss = 273.06830345\n",
      "Iteration 326, loss = 266.95132585\n",
      "Iteration 327, loss = 264.04120004\n",
      "Iteration 328, loss = 263.59363429\n",
      "Iteration 329, loss = 259.52271955\n",
      "Iteration 330, loss = 255.44928145\n",
      "Iteration 331, loss = 252.28771826\n",
      "Iteration 332, loss = 249.49477845\n",
      "Iteration 333, loss = 246.13328631\n",
      "Iteration 334, loss = 243.37879315\n",
      "Iteration 335, loss = 242.68838384\n",
      "Iteration 336, loss = 249.33210494\n",
      "Iteration 337, loss = 237.55011257\n",
      "Iteration 338, loss = 235.38291312\n",
      "Iteration 339, loss = 230.66647311\n",
      "Iteration 340, loss = 226.55906322\n",
      "Iteration 341, loss = 225.13642594\n",
      "Iteration 342, loss = 222.77559728\n",
      "Iteration 343, loss = 220.59873423\n",
      "Iteration 344, loss = 219.94869042\n",
      "Iteration 345, loss = 214.46724560\n",
      "Iteration 346, loss = 212.35136523\n",
      "Iteration 347, loss = 209.95160626\n",
      "Iteration 348, loss = 208.05637305\n",
      "Iteration 349, loss = 204.67837391\n",
      "Iteration 350, loss = 202.46559467\n",
      "Iteration 351, loss = 200.49413270\n",
      "Iteration 352, loss = 198.45829131\n",
      "Iteration 353, loss = 196.88068519\n",
      "Iteration 354, loss = 194.47340664\n",
      "Iteration 355, loss = 202.75616972\n",
      "Iteration 356, loss = 206.71353803\n",
      "Iteration 357, loss = 190.27041050\n",
      "Iteration 358, loss = 187.01099580\n",
      "Iteration 359, loss = 184.09280218\n",
      "Iteration 360, loss = 181.99332890\n",
      "Iteration 361, loss = 179.73556424\n",
      "Iteration 362, loss = 178.04037112\n",
      "Iteration 363, loss = 176.88855424\n",
      "Iteration 364, loss = 178.25129349\n",
      "Iteration 365, loss = 176.59499707\n",
      "Iteration 366, loss = 172.31888198\n",
      "Iteration 367, loss = 169.76321771\n",
      "Iteration 368, loss = 168.30314146\n",
      "Iteration 369, loss = 166.39324906\n",
      "Iteration 370, loss = 164.59276314\n",
      "Iteration 371, loss = 162.88589177\n",
      "Iteration 372, loss = 163.09598628\n",
      "Iteration 373, loss = 161.43683747\n",
      "Iteration 374, loss = 158.93586484\n",
      "Iteration 375, loss = 157.07226376\n",
      "Iteration 376, loss = 158.57848822\n",
      "Iteration 377, loss = 155.12709032\n",
      "Iteration 378, loss = 154.21124344\n",
      "Iteration 379, loss = 153.60623520\n",
      "Iteration 380, loss = 150.64734239\n",
      "Iteration 381, loss = 149.44009429\n",
      "Iteration 382, loss = 148.41128547\n",
      "Iteration 383, loss = 147.65240251\n",
      "Iteration 384, loss = 146.08480499\n",
      "Iteration 385, loss = 144.03698650\n",
      "Iteration 386, loss = 142.78685323\n",
      "Iteration 387, loss = 143.85150051\n",
      "Iteration 388, loss = 141.92426938\n",
      "Iteration 389, loss = 142.08546636\n",
      "Iteration 390, loss = 138.31208762\n",
      "Iteration 391, loss = 136.82920306\n",
      "Iteration 392, loss = 135.83581455\n",
      "Iteration 393, loss = 143.22191207\n",
      "Iteration 394, loss = 137.78156811\n",
      "Iteration 395, loss = 133.58169439\n",
      "Iteration 396, loss = 132.83142859\n",
      "Iteration 397, loss = 132.51282062\n",
      "Iteration 398, loss = 130.17644716\n",
      "Iteration 399, loss = 128.86851169\n",
      "Iteration 400, loss = 127.86473035\n",
      "Iteration 401, loss = 127.52677132\n",
      "Iteration 402, loss = 126.83287004\n",
      "Iteration 403, loss = 125.83368391\n",
      "Iteration 404, loss = 124.45193324\n",
      "Iteration 405, loss = 123.25678044\n",
      "Iteration 406, loss = 122.75112171\n",
      "Iteration 407, loss = 121.64853650\n",
      "Iteration 408, loss = 121.90568827\n",
      "Iteration 409, loss = 121.78848217\n",
      "Iteration 410, loss = 120.32981201\n",
      "Iteration 411, loss = 120.02371420\n",
      "Iteration 412, loss = 118.60788432\n",
      "Iteration 413, loss = 118.86104629\n",
      "Iteration 414, loss = 117.36765581\n",
      "Iteration 415, loss = 117.86208590\n",
      "Iteration 416, loss = 115.98106409\n",
      "Iteration 417, loss = 115.17441814\n",
      "Iteration 418, loss = 114.30062109\n",
      "Iteration 419, loss = 113.39805430\n",
      "Iteration 420, loss = 112.53793085\n",
      "Iteration 421, loss = 111.57788565\n",
      "Iteration 422, loss = 112.37976721\n",
      "Iteration 423, loss = 110.87094953\n",
      "Iteration 424, loss = 109.17687996\n",
      "Iteration 425, loss = 108.70956455\n",
      "Iteration 426, loss = 106.94055708\n",
      "Iteration 427, loss = 107.17307569\n",
      "Iteration 428, loss = 107.01058944\n",
      "Iteration 429, loss = 110.35330873\n",
      "Iteration 430, loss = 105.16289633\n",
      "Iteration 431, loss = 104.17757788\n",
      "Iteration 432, loss = 105.46187710\n",
      "Iteration 433, loss = 103.48987719\n",
      "Iteration 434, loss = 105.30145420\n",
      "Iteration 435, loss = 102.20345706\n",
      "Iteration 436, loss = 101.33813516\n",
      "Iteration 437, loss = 100.58741009\n",
      "Iteration 438, loss = 101.47722624\n",
      "Iteration 439, loss = 99.08568992\n",
      "Iteration 440, loss = 98.36527496\n",
      "Iteration 441, loss = 97.84182817\n",
      "Iteration 442, loss = 97.29045766\n",
      "Iteration 443, loss = 96.87270446\n",
      "Iteration 444, loss = 96.96148606\n",
      "Iteration 445, loss = 98.74417668\n",
      "Iteration 446, loss = 96.86969193\n",
      "Iteration 447, loss = 94.01078856\n",
      "Iteration 448, loss = 93.67958494\n",
      "Iteration 449, loss = 94.28886205\n",
      "Iteration 450, loss = 92.53210054\n",
      "Iteration 451, loss = 91.90743621\n",
      "Iteration 452, loss = 92.42535701\n",
      "Iteration 453, loss = 92.72167879\n",
      "Iteration 454, loss = 91.55805857\n",
      "Iteration 455, loss = 90.02540918\n",
      "Iteration 456, loss = 90.25322326\n",
      "Iteration 457, loss = 89.88725323\n",
      "Iteration 458, loss = 88.43994488\n",
      "Iteration 459, loss = 88.02742288\n",
      "Iteration 460, loss = 87.71066131\n",
      "Iteration 461, loss = 90.14826061\n",
      "Iteration 462, loss = 87.53440977\n",
      "Iteration 463, loss = 89.17720500\n",
      "Iteration 464, loss = 86.32991629\n",
      "Iteration 465, loss = 86.44080142\n",
      "Iteration 466, loss = 86.09949108\n",
      "Iteration 467, loss = 84.67684741\n",
      "Iteration 468, loss = 84.04246054\n",
      "Iteration 469, loss = 84.24649947\n",
      "Iteration 470, loss = 84.27721034\n",
      "Iteration 471, loss = 82.75338652\n",
      "Iteration 472, loss = 82.29746268\n",
      "Iteration 473, loss = 81.54554115\n",
      "Iteration 474, loss = 81.07746326\n",
      "Iteration 475, loss = 80.76375849\n",
      "Iteration 476, loss = 81.33566135\n",
      "Iteration 477, loss = 79.90084456\n",
      "Iteration 478, loss = 80.08422181\n",
      "Iteration 479, loss = 78.98348495\n",
      "Iteration 480, loss = 78.50781717\n",
      "Iteration 481, loss = 78.91904226\n",
      "Iteration 482, loss = 78.72494028\n",
      "Iteration 483, loss = 78.20230158\n",
      "Iteration 484, loss = 78.38772497\n",
      "Iteration 485, loss = 78.13071196\n",
      "Iteration 486, loss = 77.20324248\n",
      "Iteration 487, loss = 76.45239520\n",
      "Iteration 488, loss = 75.71705055\n",
      "Iteration 489, loss = 74.83636118\n",
      "Iteration 490, loss = 74.92829864\n",
      "Iteration 491, loss = 74.50183786\n",
      "Iteration 492, loss = 73.92410791\n",
      "Iteration 493, loss = 73.31630407\n",
      "Iteration 494, loss = 73.28536660\n",
      "Iteration 495, loss = 72.65686416\n",
      "Iteration 496, loss = 72.35133293\n",
      "Iteration 497, loss = 73.04875725\n",
      "Iteration 498, loss = 72.31002482\n",
      "Iteration 499, loss = 71.13396906\n",
      "Iteration 500, loss = 71.03963929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\buikh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "nn_model = MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=500, activation='relu', solver='adam', random_state=42, verbose=True)\n",
    "\n",
    "nn_model.fit(x_train, y_train)\n",
    "y_pred_nn = nn_model.predict(x_test)\n",
    "mse_nn = mean_squared_error(y_test, y_pred_nn)\n",
    "r2_nn = r2_score(y_test, y_pred_nn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging_model = BaggingRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "bagging_model.fit(x_train, y_train)\n",
    "y_pred_bagging = bagging_model.predict(x_test)\n",
    "mse_bagging = mean_squared_error(y_test, y_pred_bagging)\n",
    "r2_bagging = r2_score(y_test, y_pred_bagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hiển thị kết quả dự đoán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression MSE: 31.89661058812257, R2: 0.9995290188312149\n",
      "Lasso MSE: 50.7489725830702, R2: 0.9992506473264368\n",
      "Neural Network MSE: 156.93714218252276, R2: 0.9976826867404391\n",
      "Bagging MSE: 56.5997667816531, R2: 0.9991642552666172\n"
     ]
    }
   ],
   "source": [
    "print(f'Linear Regression MSE: {mse_lr}, R2: {r2_lr}')\n",
    "print(f'Lasso MSE: {mse_lasso}, R2: {r2_lasso}')\n",
    "print(f'Neural Network MSE: {mse_nn}, R2: {r2_nn}')\n",
    "print(f'Bagging MSE: {mse_bagging}, R2: {r2_bagging}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
